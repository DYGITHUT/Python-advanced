{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIT742P03D-ParsingExcelFiles.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.5",
      "name": "python3",
      "language": "python"
    }
  },
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "id": "yM5q8He4BdfJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SIT742: Modern Data Science \n",
        "**(Week 03: Data Wrangling)**\n",
        "\n",
        "---\n",
        "- Materials in this module include resources collected from various open-source online repositories.\n",
        "- You are free to use, change and distribute this package.\n",
        "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
        "\n",
        "Prepared by **SIT742 Teaching Team**\n",
        "\n",
        "---\n",
        "\n",
        "# Session 3D - Parsing Excel Files \n",
        "\n",
        "## Table of Content\n",
        "\n",
        "* Part 1. Introduction to Excel\n",
        "* Part 2. Parsing Excel with Pandas\n",
        "* Part 3. Summary\n",
        "* Part 4. Exercise\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "So far, you have learnt how to work with data in the formats that are machine readable, \n",
        "such as CSV, JSON and XML. \n",
        "The approaches used to import data in those formats are generally standard. \n",
        "However, not all data can easily be imported into Python or other programming languages without \n",
        "a fair amount of work.\n",
        "For example, with data stored in spreadsheets and PDFs. \n",
        "In these circumstances, data is generated purely for human consumption.\n",
        "The person who generated the data often tries to make it easily readable for human, \n",
        "disregarding the importance of releasing it in a machine readable format. \n",
        "\n",
        "We will provide some generic instructions on how to scrape data from excel files. \n",
        "You will find that the scraping process becomes much more difficult and time-consuming. \n",
        "But the ultimate goal stays the same, i.e., extracting data and converting it into a machine readable format. \n",
        "* * *\n",
        "\n",
        "## 1. Introduction to Excel\n",
        "\n",
        "Excel is a popular spreadsheet application originally \n",
        "developed for Windows. \n",
        "You can also find free alternatives that run on Mac OS and Linux,\n",
        "for example, LibreOffice Calc and OpenOffice Calc can both work with Excel files.\n",
        "An Excel document is also called a workbook. \n",
        "It is usually saved in a file with either .xlsx extension or .xls extension, \n",
        "depending on the Excel version you use.\n",
        "A workbook can contain multiple worksheets, each of which is a grid of cells\n",
        "where you keep and manipulate the data. \n",
        "Those cells are arranged in numbered rows and letter-named columns.\n",
        "Excel can display not only tabular data but also data like line graphs, histograms and charts.\n",
        "It also provides a set of data analysis functions for statistical, engineering and financial needs.\n",
        "Presumably, most of you know what a Excel file looks like. \n",
        "If not, please find some Excel files online and have a look or open the Excel file used in this tutorial.\n",
        "\n",
        "There are many ways of manipulating data stored in Excel spreadsheets. \n",
        "For instance, \n",
        "\"[Working with Excel Files in Python](http://www.python-excel.org/)\" contains pointers to \n",
        "the best information available about working with Excel files in Python. \n",
        "The website lists the following Python packages that deal with Excel:\n",
        "\n",
        "* `openpyxl`: Reads/writes Excel 2010 xlsx/xlsm/xltx/xltm files.\n",
        "* `xlsxwriter`: write text, numbers, formulas and hyperlinks to multiple worksheets in an Excel 2007+ XLSX file.\n",
        "* `xlrd`: Extracts data from Excel files (.xls and .xlsx, versions 2.0 onwards).\n",
        "* `xlwt`: Writes and formats Excel files compatible with Microsoft Excel versions 95 to 2003.\n",
        "* `xlutils`: Contains a set of advanced tools for manipulating Excel files (requires `xlrd` and `xlwt`).\n",
        "\n",
        "You would need to install each separately if you want to use them;\n",
        "however, in this tutorial we will use Pandas `ExcelFile` class that requires `xlrd` to demonstrate how to \n",
        "parse Excel files.\n",
        "\n",
        "Some tutorials on working with Excel files that might be of your interest:\n",
        "* [Working with Excel Spreadsheets](https://automatetheboringstuff.com/chapter12/): It utilizes openyxl to read\n",
        "data from spreadsheets. Read the following sections:\n",
        "    * Reading Excel Documents ðŸ“–\n",
        "    * Project: Reading Data from a Spreadsheet ðŸ“–\n",
        "* [How to read Excel files with Python (xlrd tutorial)](https://www.youtube.com/watch?v=p0DNcTnreuY): \n",
        "a Youtube video on extracting data from a simple Excel file. (Optional)\n",
        "\n",
        "\n",
        "This tutorial will use a running example to show\n",
        "you how to extract data from Excel spreadsheets step-by-step using Pandas.\n",
        "The example we use in this tutorial is \"[Table 2: Nutrition](http://www.unicef.org/sowc2014/numbers/documents/excel/SOWC%202014%20Stat%20Tables_Table%202.xlsx)\" from Unicef's report on \n",
        "[The State of the Worlds Children](http://www.unicef.org/sowc2014/numbers/) for 2014.\n",
        "The download link is located at the bottom of the webpage. \n",
        "Please download the Excel file, and store it in the same folder as where \n",
        "the notebook located.\n",
        "\n",
        "Our task is to extract the statistic data table on the child's issues of \n",
        "underweight, stunting, wasting and overweight prevalence in different countries.\n",
        "* * *"
      ]
    },
    {
      "metadata": {
        "id": "nSTTbyPhBdfL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Parsing Excel with Pandas\n",
        "In this section we will walk through the process of parsing our example Excel file with Pandas.\n",
        "A short tutorial on how to use Pandas `read_excel` function and the ExcelFile class  can \n",
        "be found at Pandas [webpage on IO](http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html). ðŸ“–  (Just read the section \"Reading Excel Files\".)\n",
        "\n",
        "Before we start parsing our Excel file, \n",
        "we need to first make sure the Python package `xlrd` is installed, \n",
        "as Pandas `ExcelFile` class makes use of `xlrd`. \n",
        "The `xlrd` package can be run on Linux and Mac as well as Windows.\n",
        "Here we assume you use either Linux or Mac. \n",
        "If you use Anaconda, you do not need to worry about this, \n",
        "as Anaconda includes the most popular Python packages for data analysis, including `xlrd`. \n",
        "Otherwise, you might need to install `xlrd` in order to run `read_excel`. \n",
        "To install `xlrd`, you can use [pip](https://pypi.python.org/pypi/pip), \n",
        "a Python package management system. \n",
        "In your command line, simply type\n",
        "```shell\n",
        "    pip install xlrd\n",
        "```\n",
        "\n",
        "Now to start our script, \n",
        "we need to import Pandas \n",
        "and open our Excel file by creating a Pandas `ExcelFile` object. \n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "AOJcomoABdfN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0EfUDmftBdfQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/SOWC2014.xlsx'\n",
        "\n",
        "DataSet = wget.download(link_to_data)\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SUFvKYoYBdfS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "excel_data = pd.ExcelFile('SOWC2014.xlsx')\n",
        "excel_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4NIfnA-nBdfV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By running the code above, we have loaded the Excel file as a Pandas' ExcelFile object into Python. \n",
        "\n",
        "Are we ready to parse our Excel File? Before starting to parse the file,\n",
        "we probably need to ask ourselves a couple of questions. For instance,\n",
        "* How many sheets does our Excel file have?\n",
        "* Which data sheet does contain our data? What is the name of the sheet? Or what is the index of the sheet?"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "qojy0Ey5BdfV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unlike CSV files, an Excel file can have multiple worksheets.\n",
        "For example, our Excel file contains two worksheets, one contains data notes,\n",
        "and the other contains the data we want.\n",
        "In order to get our data, we will just pull the sheet with the data we want.\n",
        "\n",
        "If your Excel file has a couple of worksheets and you can guess the index of \n",
        "the worksheet that contains the data you want, or you have been told from which\n",
        "worksheet you are going to extract data, you can directly use Panda's \n",
        "[`read_excel`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html`) \n",
        "fuction\n",
        "```python\n",
        "    pandas.read_excel()\n",
        "```\n",
        "This function reads an Excel table in a given worksheet into a Pandas DataFrame, \n",
        "where you can start further manipulating the data.\n",
        "\n",
        "However, in some cases, particularly while an Excel file has a lot of worksheets,\n",
        "it might be good to view all the sheets by their names.\n",
        "So, let's check out what the names of the sheets we have in our Excel file are:"
      ]
    },
    {
      "metadata": {
        "id": "jckmGerKBdfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "excel_data.sheet_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8t563znBdfa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are two worksheets in our Excel file.\n",
        "The one that we are looking for is \"Table 2 \". \n",
        "So, let's read the second worksheet into a Pandas DataFrame.\n",
        "Note that there is an extra space in the worksheet name.\n",
        "Without this space, running the following parsing code \n",
        "will result in the following error\n",
        "```\n",
        "    XLRDError: No sheet named <'Table 2'>\n",
        "```"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "y1og094KBdfb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Should be 'Table 2 ' \n",
        "#      Not 'Table 2'\n",
        "#If you put the 'Table 2' here, it will show XLRDError: No sheet named <'Table 2'>\n",
        "df = excel_data.parse('Table 2 ')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KTBxRXNWRzIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#It will show the rows and columns in this DataFrame\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "KwuSuDDsBdfd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have loaded the target worksheet into Python. \n",
        "There are 322 rows and 28 columns (You can use `df.shape` to \n",
        "see the dimensionality of the DataFrame).\n",
        "\n",
        "If you scroll through the output, you will notice that the loaded data table is quite messy.\n",
        "The messiness includes\n",
        "* Rows only contain missing values that are indicated by **NaN **in Pandas DataFrame.\n",
        "* Column heads are in three languages, i.e., English, French and Spanish.\n",
        "* Column heads in one language spread over multiple rows.\n",
        "* Country names also appear in three languages.\n",
        "* Notes shown in the original Excel file appear in rows towards the end of the data frame.\n",
        "\n",
        "Remember that our goal is to extract the data table in English. \n",
        "It is clear that we need to further process the data frame. \n",
        "For demonstration purpose,\n",
        "we will try to keep the example as simple as possible,\n",
        "so we will not extract column heads here. \n",
        "Instead, if you are interested in programmatically extracting column heads, \n",
        "you can try it by yourself. \n",
        "\n",
        "\n",
        "### Task 1 drop useless columns and rows\n",
        "\n",
        "We will start with removing country names in French and Spanish, \n",
        "which corresponds to remove two columns, labeled \"Unnamed: 1\" and \"Unnamed: 2\" in our data frame.\n",
        "To do this, we are going to use DataFrame's [`drop()`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) function, \n",
        "which returns a new object with labels in requested axis removed.\n",
        "We will frequently use this function later in this section."
      ]
    },
    {
      "metadata": {
        "id": "BCwRV4-lBdfe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df.drop(['Unnamed: 1', 'Unnamed: 2'], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gw953IHtBdfg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#You will find that two columns were dropped.\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rzuKqvCbBdfk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you should have 26 columns.\n",
        "Next we are going to remove all the rows and columns that are empty, i.e., only contains NaNs."
      ]
    },
    {
      "metadata": {
        "id": "aBc_MXqNBdfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#0, or â€˜indexâ€™ : Drop rows which contain missing values.\n",
        "#1, or â€˜columnsâ€™ : Drop columns which contain missing value.\n",
        "#how : {â€˜anyâ€™, â€˜allâ€™}, default â€˜anyâ€™\n",
        "#Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.\n",
        "\n",
        "#â€˜anyâ€™ : If any NA values are present, drop that row or column.\n",
        "#â€˜allâ€™ : If all values are NA, drop that row or column.\n",
        "df = df.dropna(0, how = 'all')\n",
        "df = df.dropna(1, how = 'all')\n",
        "\n",
        "#You will find that 1 columns and 77 row were dropped \n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qaJmjMNnBdfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we used the [`dropna`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) function of DataFrame. The first argument is axis (0 means row, and 1 means column),\n",
        "and the second argument indicates deleting rows/columns with all NaNs. \n",
        "We further removed 77 rows and 1 column. \n",
        "\n",
        "The printout shows that\n",
        "the very first column in the data frame only contains NaNs.\n",
        "These NaNs are row indices.\n",
        "We cannot delete it directly.\n",
        "Instead, we are going to reset the row indices with a list of integers."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "in3zuVdtBdfp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.index = range(len(df.index))\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F72YT3L1Bdfs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After resetting all the row indices, and if you print out the\n",
        "first 15 rows using the slicing method:\n",
        "```\n",
        "    df[:15]\n",
        "```\n",
        "You will find that the data we want starts from row index 9.\n",
        "The first 9 rows contain column heads in three different languages.\n",
        "As we mentioned before, to keep our script simple, we will not extract column heads here,\n",
        "rather we will delete them.\n",
        "\n",
        "Similarly, if we print out the last 50 rows,\n",
        "```\n",
        "    df[-50:]\n",
        "```\n",
        "The data we want ends at row 205. \n",
        "Therefore, we need to delete the first 9 rows and the \n",
        "last 39 rows, and then reindex all the rows left."
      ]
    },
    {
      "metadata": {
        "id": "DJI69hAwURb7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MmLhK976UYB3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[-50:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "66MrHSYfBdft",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete the first 9 rows\n",
        "df = df.drop(df.index[0:9])\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7PEbh6bUnup",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete the last 39 rows\n",
        "df = df.drop(df.index[-39:])\n",
        "df[-39:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJsORdi1UkP2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reindex rows\n",
        "df.index = range(len(df.index))\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "258pDT7oBdfw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Task 2 Set country index\n",
        "\n",
        "So far we have extracted all the records (or rows) for 196 countries in our Excel file. \n",
        "Let's set the country names as row indices, and reset the column labels."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "WGu1HvR4Bdfx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the country names as row indices\n",
        "df = df.set_index(df['TABLE 2. NUTRITION'].values)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F2gDG6nfVBUU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete \"TABLE 2. NUTRITION\" column, it is now redundant.\n",
        "df = df.drop('TABLE 2. NUTRITION', 1)\n",
        "\n",
        "# Reindex columns\n",
        "df.columns = list(range(len(df.columns))) \n",
        "\n",
        "#You will find that 'TABLE 2. NUTRITION column' was delete and index was indexed by the 'TABLE 2. NUTRITION column' vaule \n",
        "#The current column is also changed into 24\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EW94Nt3BBdfz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 3 Tidy up all columns \n",
        "\n",
        "However, those records are still messy. \n",
        "As you can see in the printout, there are a lot of NaNs, \n",
        "and cell values with both numbers and letters (e.g., \"6 x\", \" 39 x,y\",) spread over two columns.\n",
        "Therefore, we need to merge every two columns together. \n",
        "\n",
        "How can we do that?\n",
        "\n",
        "Let us have a look at the first 10 rows and 2 columns. Please recall how to use the iloc function on the WEEK 2 lab practices."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ifVsVKToBdf0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.iloc[:10, :2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7b_7nSKmBdf2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A close look at the printout will give you the following patterns:\n",
        "* If the cell contains only a float or '-', the corresponding cell value in the odd-numbered columns is \"NaN\". \n",
        "See the rows labeled \"**Afghanistan**\" (-), \"**Albania**\"(3.6), \"**Andorra**\"(-) and \"**Argentina**\"(7.2) .\n",
        "* If the original cell contains a number and a couple of letters, the cell in the even-numbered columns contains a number, and the one in the odd-numbered columns contains the letters. \n",
        "See the rows labeled \"**Algeria**\"(6,x), \"**Angola**\"(12,x). etc.\n",
        "\n",
        "Assume that we are going to merge the two cells containing a float type and letters respectively.\n",
        "\n",
        "We need a FOR loop iterating over either the odd- or the even-numbered columns.\n",
        "\n",
        "Meantime, within this FOR loop, another FOR loop is needed to iterate **over rows.**\n",
        "For each row, we check if the cell in the odd-numbered columns contains NaN.\n",
        "If it does, we then merge it with the cell in the corresponding even-numbered column on the left."
      ]
    },
    {
      "metadata": {
        "id": "ojmexJ10aCAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# It shows there 24 columns and 197 rows before we start to merge the two cells.\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUowLLT7Bdf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#A FOR loop over odd-numbered columns.\n",
        "#range (start, stop, step)\n",
        "#start\tOptional. An integer number specifying at which position to start. Default is 0\n",
        "#stop\tOptional. An integer number specifying at which position to endt.\n",
        "#step\tOptional. An integer number specifying the incrementation. Default is 1\n",
        "\n",
        "for col_idx in range(1, 24, 2): \n",
        "    # A For loop over rows\n",
        "    for row_idx in range(len(df)):\n",
        "        # A IF statement to check\n",
        "        #    1. If the cell value in the odd-numbered column is not NaN, then merge it the cell value in \n",
        "        #       the even-numbered column.\n",
        "        #    2. Otherwise, do nothing.\n",
        "        if not pd.isnull(df.iloc[row_idx, col_idx]):\n",
        "            df[col_idx-1][row_idx] = str(df[col_idx-1][row_idx]) + ' ' + str(df[col_idx ][row_idx])  \n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A6kS5qXHBdf9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next step is to remove the odd-numbered columns in the data frame, as they are redundant now.\n",
        "To do this, we are going to use DataFrame's `drop()` function again as follows"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "_e-Yr9CVBdf9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for col_idx in range(1, 24, 2): \n",
        "        df.drop(col_idx, 1, inplace = True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kDkaQ-n2BdgC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now the data is in a pretty good shape aside from the column heads. \n",
        "We can extract the column heads from the Excel file using either manual or programmatic method.\n",
        "Here we are going to do it manually. Considering that we are going to save results in an csv file. we will use the long name from the raw data \t\t\t\t\t\t\t\t\t\t\t\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "dOqIfCp0BdgE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.columns = [\"Low birthweight (%)\", \\\n",
        "              \"Early initiation of breastfeeding (%)\", \\\n",
        "              \"Exclusive breastfeeding <6 months (%)\", \\\n",
        "              \"Introduction of solid, semi-solid or soft foods 6â€“8 months (%)\", \\\n",
        "              \"Breastfeeding at age 2 (%)\", \\\n",
        "              \"Underweight (%) moderate and severe\", \\\n",
        "              \"Underweight (%) severe\", \\\n",
        "              \"Stunting (%) moderate and severe\", \\\n",
        "              \"Wasting (%) moderate and severe\",\\\n",
        "              \"Overweight (%) moderate and severe\", \\\n",
        "              \"Vitamin A supplementation, full coverage(%)\", \\\n",
        "              \"Iodized salt consumption (%)\" ]\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jf4hhAOrBdgH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we have extracted the data table from our Excel file, and put it into a Pandas DataFrame.\n",
        "The DataFrame has 197 rows and 12 columns, where rows correspond to records for individual countries\n",
        "and columns are variables (or attributes). \n",
        "Our last step is to save the data table in a CSV file."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "b9FgGZQUBdgI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv('en_final_table_2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoX9e9iUBdgL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What is the problem you get? Let's check the type of some values in the DataFrame using\n",
        "```\n",
        "    type(df.iloc[i,j])\n",
        "```\n",
        "where i indicates row index, and j indicates column index.\n",
        "You will find that DataFrame's `read_excel` method has parsed all strings and special characters,\n",
        "like '-', into Unicode objects.\n",
        "If you print the DataFrame, however, you'll get the printed version of the Unicode.\n",
        "In contrast, printing a value in a specific location, for example, \n",
        "```python\n",
        "    df.iloc[0,0]\n",
        "```\n",
        "gives you the original Unicode,\n",
        "```\n",
        "    u'\\u2013'\n",
        "```\n",
        "Therefore, you need to specify the encoding method while saving\n",
        "the DataFrame into a CSV file."
      ]
    },
    {
      "metadata": {
        "id": "rSUKMytzBdgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv('en_final_table_2.csv', sep=',', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cwtJoh4HBdgO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Summary\n",
        "Compared with the three formats discussed previously, \n",
        "Excel files are not meant to be read by programming languages, but they can still be parsed with a bit more effort. \n",
        "In this tutorial, you have learnt how to extract data from an Excel file and save\n",
        "the extracted data in a CSV file using Pandas ExcelFile class and various\n",
        "methods for manipulating DataFrame.\n",
        "\n",
        "## 4. Exercise \n",
        "1. In the introduction, we have mentioned a couple of Python libraries that can be \n",
        "use to manipulate Excel files. Here we suggest that you download and install `openpyxl`,\n",
        "and try to write your own Python script to parse the same Excel file, and store the data in the\n",
        "same format as in Section 2."
      ]
    },
    {
      "metadata": {
        "id": "AGevrg2ruwCb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<details><summary><font color='blue'><b>Click here for solution to the exercise</b></font></summary>\n",
        "```python\n",
        "!pip install openpyxl\n",
        "!pip install wget\n",
        "\n",
        "import wget\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "import numpy as np\n",
        "\n",
        "#Download file\n",
        "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/SOWC2014.xlsx'\n",
        "DataSet = wget.download(link_to_data)\n",
        "!ls\n",
        "\n",
        "#Using openpyxml load workbook\n",
        "workbook = load_workbook(filename = 'SOWC2014.xlsx')\n",
        "active_worksheet = workbook.active\n",
        "\n",
        "#Print the all sheet names of the workbook\n",
        "print(workbook.sheetnames)\n",
        "\n",
        "#Open the worksheet lable 'Table 2 ' \n",
        "sheet = workbook[\"Table 2 \"]\n",
        "\n",
        "#Due to the given worksheet has no headers or indices, we will use the below code\n",
        "dfexercise = pd.DataFrame(sheet.values)\n",
        "\n",
        "#If the given worksheet have headers or indices in the given excel file, we can refer below codes. \n",
        "\n",
        "#from itertools import islice\n",
        "#data = sheet.values\n",
        "#cols = next(data)[1:]\n",
        "#data = list(data)\n",
        "#idx = [r[0] for r in data]\n",
        "#data = (islice(r, 1, None) for r in data)\n",
        "#dfexercise = pd.DataFrame(data, index=idx, columns=cols)\n",
        "\n",
        "#Following the section 2 Data Wrangling steps\n",
        "#Before we start the Data Wrangling step\n",
        "#We should convert the None to NaN.\n",
        "#Both of them are meaning \"missing\". NaN is float type but the None is object type.\n",
        "#We should convert the None into NaN due to the NaN is more fast for parsing volums of data \n",
        "dfexercise = dfexercise.fillna(value=pd.np.nan)\n",
        "\n",
        "#Step 1, drop all NaN values which are for each rows and each columns\n",
        "dfexercise= dfexercise.dropna(0, how = 'all')\n",
        "dfexercise = dfexercise.dropna(1, how = 'all')\n",
        "\n",
        "#Step 2, drop the column with non-English\n",
        "dfexercise = dfexercise.drop([2, 3], 1)\n",
        "\n",
        "#Step 3, index\n",
        "dfexercise.index = range(len(dfexercise.index))\n",
        "\n",
        "#Step 4, delete the all row written by non-English \n",
        "# Delete the first 13 rows\n",
        "dfexercise = dfexercise.drop(dfexercise.index[0:13])\n",
        "# Delete the last 95 rows\n",
        "dfexercise = dfexercise.drop(dfexercise.index[-95:])\n",
        "\n",
        "#Step 5, Reindex again\n",
        "dfexercise.index = range(len(dfexercise.index))\n",
        "\n",
        "dfexercise = dfexercise.set_index(dfexercise[1].values)\n",
        "# Delete \"1\" column index related to the country because it is now redundant.\n",
        "dfexercise = dfexercise.drop([1], 1)\n",
        "# Reindex columns\n",
        "dfexercise.columns = list(range(len(dfexercise.columns))) \n",
        "\n",
        "#Step 7 Meger column\n",
        "for col_idx in range(1, 24, 2): \n",
        "    for row_idx in range(len(dfexercise)):\n",
        "        if not pd.isnull(dfexercise.iloc[row_idx, col_idx]):\n",
        "            dfexercise[col_idx-1][row_idx] = str(dfexercise[col_idx-1][row_idx]) + ' ' + str(dfexercise[col_idx ][row_idx])  \n",
        "#Step 8 Delete odds column            \n",
        "for col_idx in range(1, 24, 2): \n",
        "        dfexercise.drop(col_idx, 1, inplace = True)\n",
        "#Step 9 Set column index        \n",
        "dfexercise.columns = [\"Low birthweight (%)\", \\\n",
        "              \"Early initiation of breastfeeding (%)\", \\\n",
        "              \"Exclusive breastfeeding <6 months (%)\", \\\n",
        "              \"Introduction of solid, semi-solid or soft foods 6â€“8 months (%)\", \\\n",
        "              \"Breastfeeding at age 2 (%)\", \\\n",
        "              \"Underweight (%) moderate and severe\", \\\n",
        "              \"Underweight (%) severe\", \\\n",
        "              \"Stunting (%) moderate and severe\", \\\n",
        "              \"Wasting (%) moderate and severe\",\\\n",
        "              \"Overweight (%) moderate and severe\", \\\n",
        "              \"Vitamin A supplementation, full coverage(%)\", \\\n",
        "              \"Iodized salt consumption (%)\" ]\n",
        "\n",
        "#Step 10 Save the result to csv file\n",
        "dfexercise.to_csv('p03bexercise.csv', sep=',', encoding='utf-8')\n",
        "\n",
        "#Step 11 Compare the two csv file generated by different methods( xlrd and openpyxml)\n",
        "#If there are something different between them, it will show there.\n",
        "with open('en_final_table_2.csv', 'r') as t1, open('p03bexercise.csv', 'r') as t2:\n",
        "    fileone = t1.readlines()\n",
        "    filetwo = t2.readlines()\n",
        "\n",
        "with open('p03bexercise', 'w') as outFile:\n",
        "    for line in filetwo:\n",
        "        if line not in fileone:\n",
        "            outFile.write(line)\n",
        "```"
      ]
    }
  ]
}